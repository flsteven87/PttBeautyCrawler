{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "HOST = \"https://www.ptt.cc\"\n",
    "headers = {\n",
    "    'cookie': \"over18=1\"\n",
    "}\n",
    "browser.get(\"https://www.ptt.cc/bbs/Beauty/index.html\")\n",
    "soup = BeautifulSoup(browser.page_source,'lxml')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "btn_list = soup.select(\"#action-bar-container > div > div.btn-group.btn-group-paging > a\")\n",
    "btn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "btn_list[1]\n",
    "last_page = btn_list[1]['href']\n",
    "last_page\n",
    "last_page.split(\"index\")\n",
    "last_page.split(\"index\")[1]\n",
    "last_page.split(\"index\")[1].split(\".html\")\n",
    "page_str = last_page.split(\"index\")[1].split(\".html\")[0]\n",
    "page_str\n",
    "page_int = int(page_str)\n",
    "page_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PAGE_TO_CRAWL = 40\n",
    "URL_TEMPLATE = \"/bbs/Beauty/index{}.html\"\n",
    "for page in range(page_int, page_int - PAGE_TO_CRAWL, -1):\n",
    "    print(\"[DEBUG] Crawling page %s\"%(page))\n",
    "    URL = HOST + URL_TEMPLATE.format(page)\n",
    "    print(\"[DEBUG] Current page is %s\"%(URL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_list(url):\n",
    "    # input: <String> list page url\n",
    "    # output: <List> list of article urls\n",
    "    headers = {\n",
    "        'cookie': \"over18=1\"\n",
    "    }\n",
    "    res = requests.get(url, headers=headers)\n",
    "    #print(\"[DEBUG] res page is %s\"%(res.text))\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    a_tags = soup.select(\"#main-container > div.r-list-container.action-bar-margin.bbs-screen > div > div.title > a\")\n",
    "    article_urls = [HOST + link['href'] for link in a_tags]\n",
    "    return article_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crawl_list(\"https://www.ptt.cc/bbs/Beauty/index2118.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_page(url):\n",
    "    # input: <String> article page url\n",
    "    # output: <Dictionary> article object\n",
    "    print(\"[DEBUG] crawl_page at %s\"%(url))\n",
    "    article_dict = {}\n",
    "    \n",
    "    headers = {\n",
    "        'cookie': \"over18=1\"\n",
    "    }\n",
    "    res = requests.get(url, headers=headers)\n",
    "    #print(\"[DEBUG] res page is %s\"%(res.text))\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    meta_list = soup.select(\"#main-content > div > span.article-meta-value\") # [kohanchen (kohanchen), Gossiping...]\n",
    "    article_dict['author'] = meta_list[0].text\n",
    "    article_dict['board'] = meta_list[1].text\n",
    "    article_dict['title'] = meta_list[2].text\n",
    "    article_dict['dt'] = meta_list[3].text\n",
    "    \n",
    "    article_dict['ip'] = soup.select_one(\"#main-content\").text.split(\"發信站: 批踢踢實業坊(ptt.cc), 來自: \")[1].split(\"\\n※ 文章網址:\")[0].strip()\n",
    "    \n",
    "    # 刪除不需要的elemets....\n",
    "    for meta in meta_list:\n",
    "        meta.extract()\n",
    "    for meta in soup.select(\"#main-content > div > span.article-meta-tag\"):\n",
    "        meta.extract()\n",
    "    for push in soup.select(\"div.push\"):\n",
    "        push.extract() \n",
    "    for push in soup.select(\"span.f2\"):\n",
    "        push.extract()\n",
    "\n",
    "    article_dict['content'] = soup.select_one(\"#main-content\").text.strip()\n",
    "    \n",
    "    return article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging # https://docs.python.org/2/library/logging.html\n",
    "logging.basicConfig(filename='crawler.log',level=logging.DEBUG)\n",
    "logger = logging.getLogger(\"crawler\")\n",
    "logger.debug('This message should go to the log file')\n",
    "\n",
    "\n",
    "PAGE_TO_CRAWL = 3\n",
    "URL_TEMPLATE = \"/bbs/Beauty/index{}.html\"\n",
    "for page in range(page_int, page_int - PAGE_TO_CRAWL, -1):\n",
    "    print(\"[DEBUG] Crawling page %s\"%(page))\n",
    "    URL = HOST + URL_TEMPLATE.format(page)\n",
    "    print(\"[DEBUG] Current page is %s\"%(URL))\n",
    "    for article in crawl_list(URL):\n",
    "        try:\n",
    "            print(article[37:42])\n",
    "            res = requests.get(article)\n",
    "            soup = BeautifulSoup(res.text,'lxml')\n",
    "            photo = soup.select(\"#main-content > a\")\n",
    "            for count in range(1,len(photo)):\n",
    "                print(photo[count]['href'])\n",
    "                urlretrieve(photo[count]['href'], \"%s_photo_%d.jpg\" %(article[37:42],count))\n",
    "        except Exception:\n",
    "            logger.error(\"crawl_page failed, url is %s\"%(article))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
